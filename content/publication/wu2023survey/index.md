---
title: "A survey on large language models for recommendation"
authors:
- Likang Wu
- Zhi Zheng
- Zhaopeng Qiu
- Hao Wang
- Hongchao Gu
- Tingjia Shen
- Chuan Qin
- Chen Zhu
- Hengshu Zhu
- Qi Liu
- Hui Xiong
- Enhong Chen

date: "2023-05-31"
doi: "10.48550/arXiv.2305.19860"

publication_types: ["article"]
publication: "arXiv"
publication_short: ""

abstract: Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation.

---